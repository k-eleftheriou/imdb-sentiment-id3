{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ID3 Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3:\n",
    "\n",
    "    class Node:\n",
    "        \n",
    "        def __init__(self, feature_index=None, left=None, right=None, class_=None):\n",
    "            self.feature_index = feature_index\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.class_ = class_\n",
    "\n",
    "    # Creates a new ID3 Decision Tree classifier.\n",
    "    def __init__(self, max_depth=20, min_samples_split=3):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        \n",
    "    def fit(self, data_frame, classes, features):\n",
    "        \"\"\"\n",
    "        Fits the ID3 Decision Tree classifier to the provided data.\n",
    "\n",
    "        Args:\n",
    "            data_frame: The input data as a DataFrame.\n",
    "            labels (array): The labels for the input data.\n",
    "            features (array): The features to be used for building the tree.\n",
    "        \"\"\"\n",
    "        self.root = self.build_tree(data_frame, classes, features, self.max_depth, self.min_samples_split)\n",
    "\n",
    "    def build_tree(self, train_dataframe, classes, features, max_depth, min_examples, default_class=None):\n",
    "        \"\"\"\n",
    "        Builds the ID3 Decision Tree recursively.\n",
    "\n",
    "        Args:\n",
    "            train_dataframe: The input data as a DataFrame.\n",
    "            classes (array): The labels for the input data.\n",
    "            features (array): The features to be used for building the tree.\n",
    "            max_depth (int): The maximum depth of the tree.\n",
    "            min_examples (int): The minimum number of samples required to split a node.\n",
    "            default_class: The default class value to be assigned if a leaf node is reached.\n",
    "\n",
    "        Returns:\n",
    "            Node: The root node of the built tree.\n",
    "        \"\"\"\n",
    "        pos_percentage, neg_percentage = self.calculate_class_percentages(train_dataframe)\n",
    "\n",
    "        if max_depth == 0:\n",
    "            return self.Node(class_=1 if pos_percentage > neg_percentage else 0 if pos_percentage < neg_percentage else default_class)\n",
    "\n",
    "        if pos_percentage == 1:\n",
    "            return self.Node(class_=1)\n",
    "        if neg_percentage == 1:\n",
    "            return self.Node(class_=0)\n",
    "\n",
    "        if pos_percentage >= 0.90 and neg_percentage < 0.10:\n",
    "            return self.Node(class_=1)\n",
    "        if pos_percentage < 0.10 and neg_percentage >= 0.90:\n",
    "            return self.Node(class_=0)\n",
    "\n",
    "        if len(train_dataframe) == 0 or len(features) == 0:\n",
    "            return self.Node(class_=default_class)\n",
    "\n",
    "        default_class = 1 if pos_percentage > neg_percentage else 0 if pos_percentage < neg_percentage else random.randint(0, 1)\n",
    "        b_feature_index = self.find_max_ig_feature(features, classes)\n",
    "\n",
    "        new_features = np.delete(features, b_feature_index, axis=1)\n",
    "\n",
    "        left_nodes = train_dataframe[train_dataframe[:, b_feature_index] == 1]\n",
    "        left_classes = left_nodes[:, -1]\n",
    "        left_features = left_nodes[:, :-1]\n",
    "\n",
    "        right_nodes = train_dataframe[train_dataframe[:, b_feature_index] == 0]\n",
    "        right_classes = right_nodes[:, -1]\n",
    "        right_features = right_nodes[:, :-1]\n",
    "\n",
    "        if len(left_nodes) < min_examples and len(right_nodes) < min_examples:\n",
    "            return self.Node(class_=default_class)\n",
    "\n",
    "        left_tree = self.build_tree(left_nodes, left_classes, left_features, max_depth - 1, min_examples, default_class)\n",
    "        right_tree = self.build_tree(right_nodes, right_classes, right_features, max_depth - 1, min_examples, default_class)\n",
    "\n",
    "        return self.Node(b_feature_index, left_tree, right_tree)\n",
    "\n",
    "    \n",
    "    def predict(self, root, test):\n",
    "        \"\"\"\n",
    "        Predicts the class label for the given test data.\n",
    "\n",
    "        Args:\n",
    "            root (Node): The root node of the decision tree.\n",
    "            test (array): The input test data.\n",
    "\n",
    "        Returns:\n",
    "            int: The predicted class label.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the current node is None, return None (base case)\n",
    "        if root is None:\n",
    "            return None\n",
    "\n",
    "        # If the current node is a leaf node, return the assigned class\n",
    "        if root.class_ is not None:\n",
    "            return root.class_\n",
    "\n",
    "        # Get the feature index to check from the current node\n",
    "        feature_index = root.feature_index\n",
    "\n",
    "        # Check the value of the feature in the test data\n",
    "        value = test[feature_index]\n",
    "\n",
    "        # Traverse the left subtree if the feature value is 1\n",
    "        if value == 1:\n",
    "            return self.predict(root.left, test)\n",
    "\n",
    "        # Traverse the right subtree if the feature value is 0\n",
    "        elif value == 0:\n",
    "            return self.predict(root.right, test)\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_class_percentages(self, data_frame):\n",
    "        \"\"\"\n",
    "        Calculates the percentages of positive and negative labels in a dataset.\n",
    "\n",
    "        Args:\n",
    "            data_frame (array-like): The input data.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the percentage of positive labels and the percentage of negative labels.\n",
    "        \"\"\"\n",
    "\n",
    "        pos_percentage = 0\n",
    "        neg_percentage = 0\n",
    "\n",
    "        # Extract the class labels from the data_frame\n",
    "        classes = np.array([df[-1] for df in data_frame])\n",
    "\n",
    "        # Count the occurrences of positive and negative classes\n",
    "        pos_count = np.sum(classes == 1)\n",
    "        neg_count = np.sum(classes == 0)\n",
    "\n",
    "        all_categories = pos_count + neg_count\n",
    "\n",
    "        # Calculate the percentages of positive and negative classes\n",
    "        if all_categories != 0:\n",
    "            pos_percentage = pos_count / all_categories\n",
    "            neg_percentage = neg_count / all_categories\n",
    "\n",
    "        # Return the percentages of positive and negative classes\n",
    "        return pos_percentage, neg_percentage\n",
    "\n",
    "    \n",
    "    def find_max_ig_feature(self, features, classes):\n",
    "        \"\"\"\n",
    "        Finds the feature with the maximum information gain.\n",
    "\n",
    "        Args:\n",
    "            features (array): The features to be used for calculating information gain.\n",
    "            classes (array): The labels for the input data.\n",
    "\n",
    "        Returns:\n",
    "            int: The index of the feature with the maximum information gain.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the overall entropy based on the classes\n",
    "        overall_entropy = self.calculate_entropy(classes)\n",
    "\n",
    "        # List to store the calculated information gains for each feature\n",
    "        igs = []\n",
    "\n",
    "        # Iterate over each feature index\n",
    "        for i in range(len(features[0])):\n",
    "            # Get the i-th column of every feature\n",
    "            feature_i = features[:, i]\n",
    "\n",
    "            # Calculate the information gain for this specific feature\n",
    "            ig_feat_i = self.calculate_ig(classes, feature_i, overall_entropy)\n",
    "            igs.append(ig_feat_i)\n",
    "\n",
    "        # Return the index of the feature with the maximum information gain\n",
    "        return np.argmax(igs)\n",
    "\n",
    "    \n",
    "    def calculate_entropy(self, classes):\n",
    "        \"\"\"\n",
    "        Calculates the entropy of a set of labels.\n",
    "\n",
    "        Args:\n",
    "            classes (array): The labels for the input data.\n",
    "\n",
    "        Returns:\n",
    "            float: The entropy.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the unique classes in the array\n",
    "        unique_classes = np.unique(classes)\n",
    "\n",
    "        entropy = 0\n",
    "        # Calculate the entropy for each class\n",
    "        for c in unique_classes:\n",
    "            # Count the number of times the class occurs\n",
    "            class_count = np.count_nonzero(classes == c)\n",
    "            # Calculate the probability of the class\n",
    "            p_c = class_count / len(classes)\n",
    "            # Calculate the entropy for this class\n",
    "            entropy += -p_c * math.log(p_c, 2)\n",
    "\n",
    "        return entropy\n",
    "\n",
    "        \n",
    "    def calculate_ig(self, classes, feature, entropy):\n",
    "        \"\"\"\n",
    "        Calculates the information gain for a given feature.\n",
    "\n",
    "        Args:\n",
    "            classes (array): The labels for the input data.\n",
    "            feature (array): The feature to be used for calculating information gain.\n",
    "            entropy (float): The entropy of the labels.\n",
    "\n",
    "        Returns:\n",
    "            float: The information gain.\n",
    "        \"\"\"\n",
    "\n",
    "        unique_classes = np.unique(classes)\n",
    "        unique_feature_values = np.unique(feature)\n",
    "\n",
    "        Hc_feature = 0\n",
    "        # Calculate the probability of each unique feature value\n",
    "        pf = np.bincount(feature) / len(feature)\n",
    "\n",
    "        # Iterate over the unique feature values\n",
    "        for feat in unique_feature_values:\n",
    "            # Find the indices of the feature values that match the current value\n",
    "            indices = np.where(feature == feat)[0]\n",
    "            # Get the corresponding class values for the feature values that match the current value\n",
    "            classes_of_feat = classes[indices]\n",
    "            for c in unique_classes:\n",
    "                # Calculate the probability of class c given feature value feat\n",
    "                pcf = np.count_nonzero(classes_of_feat == c) / len(classes_of_feat)\n",
    "                if pcf != 0:\n",
    "                    temp_H = -pf[feat] * pcf * math.log(pcf, 2)\n",
    "                    Hc_feature += temp_H\n",
    "\n",
    "        ig = entropy - Hc_feature\n",
    "        return ig\n",
    "   \n",
    "\n",
    "    def score_tree(self, test_dataframe):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy, precision, recall, and F1 score of the decision tree on a test dataset.\n",
    "\n",
    "        Args:\n",
    "            test_dataframe (array): The test dataset.\n",
    "\n",
    "        Returns:\n",
    "            float: The accuracy of the decision tree.\n",
    "            float: The precision of the decision tree.\n",
    "            float: The recall of the decision tree.\n",
    "            float: The F1 score of the decision tree.\n",
    "        \"\"\"\n",
    "\n",
    "        true_positives = 0\n",
    "        true_negatives = 0\n",
    "        false_positives = 0\n",
    "        false_negatives = 0\n",
    "\n",
    "        for test in test_dataframe:\n",
    "            # Getting the tree's prediction for the specific test (where test is the review of the movie in binary)\n",
    "            prediction = self.predict(self.root, test)\n",
    "            # The actual class of the review stored in the test dataframe\n",
    "            actual_class = test[-1]\n",
    "\n",
    "            if actual_class == 1 and prediction == 1:\n",
    "                true_positives += 1\n",
    "            elif actual_class == 0 and prediction == 0:\n",
    "                true_negatives += 1\n",
    "            elif actual_class == 1 and prediction == 0:\n",
    "                false_negatives += 1\n",
    "            elif actual_class == 0 and prediction == 1:\n",
    "                false_positives += 1\n",
    "\n",
    "        # Calculating the results (accuracy, precision, recall, f1 score)\n",
    "        accuracy = round((true_positives + true_negatives) / (true_negatives + true_positives + false_positives + false_negatives), 4)\n",
    "        precision = round(true_positives / (true_positives + false_positives), 4)\n",
    "        recall = round(true_positives / (true_positives + false_negatives), 4)\n",
    "        f1_score = round((2 * precision * recall) / (precision + recall), 4)\n",
    "\n",
    "        return accuracy, precision, recall, f1_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the binary vector data for both training testing, by loading the Keras IMDb Dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bin_data(m, n, k, test=0):\n",
    "    \"\"\"\n",
    "    Creates the binary vector data for training or testing.\n",
    "\n",
    "    Args:\n",
    "        m (int): The number of most frequent words to be included as features.\n",
    "        n (int): The number of most frequent words to be skipped.\n",
    "        k (int): The number of rarest words to be skipped.\n",
    "        test (int, opt): Indicator for creating test data. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        numpy array: The binary vector data.\n",
    "        numpy array: The class labels.\n",
    "        numpy array: The features.\n",
    "    \"\"\"\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=m - k, skip_top=n)\n",
    "\n",
    "    word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "    index2word = {i + 3: word for word, i in word_index.items()}\n",
    "    index2word[0] = '[pad]'\n",
    "    index2word[1] = '[bos]'\n",
    "    index2word[2] = '[oov]'\n",
    "    x_train = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])\n",
    "    x_test = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])\n",
    "\n",
    "    data_frame = []\n",
    "    \n",
    "    if test == 0:\n",
    "        vocabulary = np.array([word for text in x_train for word in text.split()])\n",
    "        vocabulary = np.unique(vocabulary)\n",
    "        for text, class_ in zip(x_train, y_train):\n",
    "            splitted_text = text.split()\n",
    "            features = np.isin(vocabulary, splitted_text).astype(bool)\n",
    "            data_frame.append(np.concatenate([features, [class_]]))\n",
    "        data_frame = np.array(data_frame)\n",
    "    else:\n",
    "        vocabulary = np.array([word for text in x_test for word in text.split()])\n",
    "        vocabulary = np.unique(vocabulary)\n",
    "        for text, class_ in zip(x_test, y_test):\n",
    "            splitted_text = text.split()\n",
    "            features = np.isin(vocabulary, splitted_text).astype(bool)\n",
    "            data_frame.append(np.concatenate([features, [class_]]))\n",
    "        data_frame = np.array(data_frame)\n",
    "\n",
    "    # Extracting the class labels\n",
    "    classes = data_frame[:, -1]\n",
    "    # Extracting the features\n",
    "    features = data_frame[:, :-1]\n",
    "\n",
    "    print(\"Data is now read!\")\n",
    "\n",
    "    return data_frame, classes, features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Creating binary data...\")\n",
    "    data_frame, classes, features = create_bin_data(m=850, n=45, k=1)\n",
    "    test_data_frame, test_classes, test_features = create_bin_data(m=850, n=45, k=1, test=1)\n",
    "\n",
    "    id3 = ID3()\n",
    "    id3.fit(data_frame, classes, features)\n",
    "    accuracy, precision, recall,f1_score = id3.score_tree(test_data_frame)\n",
    "    \n",
    "    print(\"Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1_score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
